---
date: "2021-09-15"
lastmod: "2021-10-26"
draft: false
title: Systems of Linear Equations
weight: 20
bibliography: references.bib
---

{{% notice note %}}
Disclaimer: None of the values used represent real-world examples.
{{% /notice %}}

## Designing a System of Linear Equations

How exactly is linear algebra a key component in many optimization and 
statistics problems? The answer is solving the title of this section. 
In most optimization problems, we want to find the best way to allocate
resources to accomplish a goal. Not the largest/smallest nor longest/shortest
way, but an **optimal solution.** In this example, a freight rail company wants 
to transport commodities $(x_1, x_2, x_3)$ from one place to another while 
maximizing revenue. However, they can't transport entire inventories of each 
commodity at once due to **constraints (limitations, conditions, or restrictions):**

* An order of commodities $x_1, x_2,$ and $x_3$ weigh 1, 2, and 3 tons respectively 
as the locomotive weight capacity is 1,250 tons. 
* Commodities $x_1, x_2,$ and $x_3$ require 2, 1, and 4 freight cars each with
1,800 freight cars available.
* Commodities $x_2$ and $x_3$ must total 300 units.

\begin{align}
  \left[\begin{array}{@{}ccc|c@{}}
    1 & 2 & 3 & 1250 \\
    2 & 1 & 4 & 1800 \\
    0 & 1 & 1 & 300
  \end{array}\right] \leftrightarrow
  \left[\begin{array}{@{}ccc|c@{}}
    x_1 & 2x_2 & 3x_3 & 1250 \\
    2x_1 & x_2 & 4x_3 & 1800 \\
    & x_2 & x_3 & 300
  \end{array}\right]
\end{align}

> On the left-hand side (LHS), as indicated by the vertical line, you have a 
coefficient matrix which is made up of the unit values from your commodities. 
$x_1, x_2, x_3$ are referred to as **decision variables** since we're trying to 
figure out the quantity needed to fulfill our linear equations. On the right-hand 
side (RHS), we have our constraints.

## Matrix Arithmetic
### Matrix Addition

\begin{align}
  \begin{bmatrix}
    1 & 2 \\
    5 & 6
  \end{bmatrix} +
  \begin{bmatrix}
    5 & 4 \\
    1 & 1
  \end{bmatrix} =
  \begin{bmatrix}
    6 & 6 \\
    6 & 7
  \end{bmatrix}
\end{align}

Matrix addition isn't that much different from normal addition except that you're
going based on corresponding positions. For example, $A_{11}$ is only being added by
$B_{11}$. In this case, 1 is the element in $A$ and 5 is the element in $B$.

```{python}
import numpy as np

A = np.matrix([[1, 2],
             [5, 6]])
B = np.matrix([[5, 4],
             [1, 1]])

np.add(A, B)
```

### Matrix Scalar Multiplication

\begin{align}
  3
  \begin{bmatrix}
    1 & 2 \\
    5 & 6
  \end{bmatrix} = 
  \begin{bmatrix}
    3 & 6 \\
    15 & 18
  \end{bmatrix}
\end{align}

```{python}
3 * A
```

### Matrix Multiplication

{{% notice warning %}}
Pay extra attention in this section as many problems in OR involve some form of 
matrix multiplication.
{{% /notice %}}

Matrix multiplication isn't as straightforward as regular multiplication due to 
several conditions: 

* The total columns in Matrix $A$ must equal the total rows in Matrix $B$.
* The total rows from the final Matrix $C$ must equal the total rows in Matrix $A$.
* The total columns from the final Matrix $C$ must equal the total columns in Matrix $B$.

<details>
<summary> Normal Explanation (click to expand)</summary>
In layman terms, an $m \times n$ matrix and an $n \times p$ matrix results in
an $m \times p$ matrix. Because the order of the matrices matter, matrix 
multiplication isn't "commutative." If you tried to perform matrix 
multiplication with the matrices in a different order, an $n \times p$ matrix 
and an $m \times n$ matrix results in an $n \times n$ matrix. To simplify this:
$AB = C \neq BA = C$. When finding the elements in $C$, you take the dot product
of row and column vectors based on the ijth entries of a matrix. 

\begin{align}
  \begin{bmatrix}
    a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \\
    a_{41} & a_{42} & a_{43}
  \end{bmatrix}
  \begin{bmatrix}
    b_{11} & b_{12} \\
    b_{21} & b_{22} \\
    b_{31} & b_{32}
  \end{bmatrix} =
  \begin{bmatrix}
    c_{11} & c_{12} \\
    c_{21} & c_{22} \\
    c_{31} & c_{32} \\
    c_{41} & c_{42}
  \end{bmatrix}
\end{align}

```{python}
np.random.seed(1234)

A = np.random.randint(20, size = (10, 3))
B = np.random.randint(20, size = (3, 5))

print(A)
print(B)
np.matmul(A, B)
```

If we want to find the dot product at $c_{21}$, we'd take the dot product of 
the 2nd row and the 1st column. Essentially, **matrix multiplication is another way
to notate linear combinations.** 

</details>

<details>
<summary>Abstraction and Generalization Explanation (click to expand)</summary>
\begin{align}
  A & \in R^{m \times n} \\
  B & \in R^{n \times p} \\
  C = AB & \in R^{m \times p} \\
  c_{ij} & = \sum_{j = 1}^n a_{ij}b_{jk}
\end{align}
</details>

## Solving a System of Linear Equations

Normally, this problem set would be solved through [gaussian elimination and back 
substitution](https://www.math.usm.edu/lambers/mat610/sum10/lecture4.pdf). 
Basically, you're eliminating unknown variables by manipulating equations with
elementary row operations (EROs). For example, if I wanted to get rid of $x_1$ in the 
2nd equation, I'd replace the 2nd equation by the sum of multiplying the 1st equation 
by -2 and the 2nd equation itself. You do this repeatedly until an equation has a 
single unknown. The final matrix after conducting EROs and back substitution 
leaves it in **reduced echelon form (REF).**

\begin{align}
  \left[\begin{array}{@{}ccc|c@{}}
    -2 & -4 & -6 & -2500 \\
    2 & 1 & 4 & 1800 \\ \hline
    0 & -3 & -2 & -700 
  \end{array}\right] \rightarrow
    \begin{bmatrix}
    1 & 2 & 3 & 150 \\
    0 & -3 & -2 & -700 \\
    0 & 1 & 1 & 300
  \end{bmatrix} \rightarrow
  \begin{bmatrix}
    1 & 2 & 3 & 150 \\
    0 & -3 & -2 & -700 \\
    0 & 0 & 1 & 200
  \end{bmatrix}
\end{align}

{{% notice note %}}
You can reduce the final matrix even further by applying EROs until you are left
with a single unknown (and a coefficient of 1) in each equation. Imagine a 
coefficient matrix with a diagonal line of 1s (typically from the upper left to 
bottom right) while every other element is listed as 0. This is known as 
**reduced row echelon form (RREF).** This allows a clearer way to find the 
solution to a system of linear equations without having to constantly perform 
back substitution on a matrix in REF (increasing the likelihood of mistakes). 
{{% /notice %}}

$x_3$ results in 200 from replacing the 3rd equation by finding the sum of equation 2 and 
multiplying the 3rd equation by 3 (to eliminate $x_2$). For the back substitution, 
plug in 200 into $x_3$ in equation 2 to solve for $x_2$ resulting in 100. Plug 
both values into their respective variables in equation 1 to get 450 for $x_1$. 
As a result, $(x_1, x_2, x_3) = (450, 100, 200)$. As our systems of linear equations 
begin to scale in size, it isn't practical to solve them by hand. That's why 
Python has `sympy` and `numpy.linalg` to obtain a matrix's RREF.

```{python}
import pandas as pd
import sympy as sp
import numpy.linalg as la

# Method 1
M = sp.Matrix([
  [1, 2, 3, 1250],
  [2, 1, 4, 1800],
  [0, 1, 1, 300]
])

M.rref()
```

```{python}
# Method 2
data = pd.read_csv('sample.csv')
print(data)
```

```{python}
## Swap rows and columns to fit system of linear equations mold
aT = np.transpose(data.iloc[:, 1:]) 
aT = np.matrix(aT)
b = np.array([1250, 1800, 300])

## Applies elementary row operations to a system of linear equations
def eroLEQ(matrix, constraints):
  A = matrix
  b = constraints
  
  ### Check if the matrix has the same number of rows and columns
  if A.shape[0] == A.shape[1]:
    #### la.solve only accepts square matrices in the 1st parameter
    return la.solve(A, b)
  else:
    #### la.lstsq for non-square matrices
    return la.lstsq(A, b, rcond = None)[0]
  
eroLEQ(aT, b)
```

## Putting it all Together in Gurobi

You probably want to know the purpose of the solution is. In OR, we want to
find the unknown quantities for our decision variables leading to the 
best way to achieve organizational goals. For example, this unique solution 
is the optimal solution to maximize our **objective function** (i.e. revenue). 

* $x_1$ generates \$1 per ton
* $x_2$ generates \$3 per ton
* $x_3$ generates \$4 per ton

<details>
<summary>Normal Solution (click to expand)</summary>
\begin{align}
  \text{Maximize} ~ x_1 + 3x_2 + 4x_3 ~ s.t. \\
    x_1 + 2x_2 + 3x_3 & = 1250 \\
    2x_1 + x_2 + 4x_3 & = 1800 \\
    x_2 + x_3 & = 300 \\
    x_1, x_2, x_3 & \geq 0
\end{align}

```{python}
import gurobipy as gp
from gurobipy import GRB

# Create model
T = gp.Model("Commodity")

# Create decision variables
x1 = T.addVar(name="Coal")
x2 = T.addVar(name="Intermodal")
x3 = T.addVar(name="Agricultural")

# Set objective function
T.setObjective(x1 + 3*x2 + 4*x3, gp.GRB.MAXIMIZE)

# Create constraints
T.addConstr(x1 + 2*x2 + 3*x3 == 1250, "Weight")
T.addConstr(2*x1 + x2 + 4*x3 == 1800, "Capacity")
T.addConstr(x2 + x3 == 300, "Mixture")

# Find solution
T.optimize()

# Display solution
for i in T.getVars():
  print(f'{i.varName}: {int(i.x)} orders of {i.varName.lower()} products.')
  
print(f'Optimal Total Revenue: ${int(T.objVal):,}')
```
</details>

<details>
  <summary>Abstracted and Generalized Formulation (click to expand)</summary>
  \begin{align}
    Max ~ & \sum_{j =1}^n a_jx_j ~ s.t. \\
    \sum_{j = 1}^n a_{ij}x_j & = B_i, \forall i \in 1, \dots, m \\
    x_j & \geq 0, \forall j \in 1, \dots, n
  \end{align}
</details>

The phrase **"garbage in, garbage out"** applies to OR modeling as a whole so its 
important to **trust, but verify** your data and solution. The next section 
covers how to do so.

## Terminology

* **Constraints**: Restrictions on the quantities of decision variables 
available.
* **Decision Variables:** Variables whose quantities affect the objective function 
in a decision-making process.
* **Dot Product:** The multiplication of two vectors with the same number of elements.
* **Objective Function:** A function that looks to maximize/minimize some value.
* **Optimal Solution:** Any feasible solution that maximizes/minimizes the objective value.
* **Reduced Echelon Form (REF):** According to @margalit_rabinoff_2019, a matrix is in REF if:
  + All nonzero rows are above rows of all 0s.
  + The leading nonzero entry of a row is to the *right* of the 1st nonzero entry of the row above.
  + Below the pivot, all entries are 0.
* **Reduced Row Echelon Form (RREF):** According to @margalit_rabinoff_2019, a 
matrix is in RREF if its in REF and:
  + The pivot is equal to 1.
  + The pivot is the *only* nonzero entry in its column.

## References