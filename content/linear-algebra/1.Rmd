---
date: "2021-09-15"
lastmod: "2021-10-14"
draft: false
title: Systems of Linear Equations
weight: 20
bibliography: references.bib
---

{{% notice warning %}}
Disclaimer: None of the values used represent real-world examples.
{{% /notice %}}

## Designing a System of Linear Equations

How exactly is linear algebra a key component in many optimization and 
statistics problems? The answer is solving the title of this section. 
In most optimization problems, we want to find the best way to allocate
resources to accomplish a goal. Not the largest/smallest nor longest/shortest
way, but an **optimal solution.** In this example, a freight rail company wants 
to transport commodities $(x_1, x_2, x_3)$ from one place to another while 
maximizing profits. However, they can't transport entire inventories of each 
commodity at once due to **constraints (limitations, conditions, or restrictions):**

* An order of commodities $x_1, x_2,$ and $x_3$ weigh 1, 2, and 3 tons respectively 
as the locomotive weight capacity is 1,250 tons. 
* Commodities $x_1, x_2,$ and $x_3$ require 2, 1, and 4 freight cars each with
1,800 freight cars available.
* Commodities $x_2$ and $x_3$ must total 300 units.

\begin{align}
  \left[\begin{array}{@{}ccc|c@{}}
    1 & 2 & 3 & 1250 \\
    2 & 1 & 4 & 1800 \\
    0 & 1 & 1 & 300
  \end{array}\right] \leftrightarrow
  \left[\begin{array}{@{}ccc|c@{}}
    x_1 & 2x_2 & 3x_3 & 1250 \\
    2x_1 & x_2 & 4x_3 & 1800 \\
    & x_2 & x_3 & 300
  \end{array}\right]
\end{align}

> On the left-hand side (LHS), as indicated by the vertical line, you have a 
coefficient matrix which is made up of the unit values from your commodities. 
$x_1, x_2, x_3$ are referred to as **decision variables** since we're trying to 
figure out the quantity needed to fulfill our linear equations. On the right-hand 
side (RHS), we have our constraints.

## Matrix Arithmetic
### Matrix Addition

\begin{align}
  \begin{bmatrix}
    1 & 2 \\
    5 & 6
  \end{bmatrix} +
  \begin{bmatrix}
    5 & 4 \\
    1 & 1
  \end{bmatrix} =
  \begin{bmatrix}
    6 & 6 \\
    6 & 7
  \end{bmatrix}
\end{align}

Matrix addition isn't that much different from normal addition except that you're
going based on corresponding positions. For example, $A_{1,1}$ is only being added by
$B_{1,1}$. In this case, 1 is the element in $A$ and 5 is the element in $B$.

```{python}
import numpy as np

A = np.matrix([[1, 2],
             [5, 6]])
B = np.matrix([[5, 4],
             [1, 1]])

np.add(A, B)
```

### Matrix Scalar Multiplication

\begin{align}
  3
  \begin{bmatrix}
    1 & 2 \\
    5 & 6
  \end{bmatrix} = 
  \begin{bmatrix}
    3 & 6 \\
    15 & 18
  \end{bmatrix}
\end{align}

```{python}
3 * A
```

### Matrix Multiplication

{{% notice note %}}
Disclaimer: Pay extra attention in this section as many problems in OR involve 
some form of matrix multiplication.
{{% /notice %}}

Matrix multiplication isn't as straightforward as regular multiplication due to 
several conditions: 

* The total columns in Matrix $A$ must equal the total rows in Matrix $B$
* The total rows from the final Matrix $C$ must equal the total rows in Matrix $A$
* The total columns from the final Matrix $C$ must equal the total columns in Matrix $B$

In layman terms, an $m \times n$ matrix and an $n \times p$ matrix results in
an $m \times p$ matrix. Because the order of the matrices matter, matrix 
multiplication isn't "commutative." If you tried to perform matrix 
multiplication with the matrices in a different order, an $n \times p$ matrix an 
$m \times n$ matrix results in an $n \times n$ matrix. To simplify this:
$AB = C \neq BA = C$. When finding the elements in $C$, they're based on 
**dot product.** 

```{python}
np.random.seed(1234)

A = np.random.randint(20, size = (10, 3))
B = np.random.randint(20, size = (3, 5))

print(A)
print(B)
np.matmul(A, B)
```

\begin{align}
  A_{2,1}B_{1,3} + A_{2,2}B_{2,3} + \dots + A_{2,n}B_{m,3} & = C_{2,3} \\
  \begin{bmatrix}
    12 & 15 & 17
  \end{bmatrix} 
  \begin{bmatrix}
    11 \\
    19 \\
    17
  \end{bmatrix} = 
  12(11) + 15(19)+17(17) & = 706 
\end{align}

Essentially, **matrix multiplication is a set of linear combinations.**

## Solving a System of Linear Equations

Normally, this problem set would be solved through [gaussian elimination and back 
substitution](https://www.math.usm.edu/lambers/mat610/sum10/lecture4.pdf). 
Basically, you're eliminating unknown variables by manipulating equations with
elementary row operations (EROs). For example, if I wanted to get rid of $x_1$ in the 
2nd equation, I'd replace the 2nd equation by the sum of multiplying the 1st equation 
by -2 and the 2nd equation itself. You do this repeatedly until an equation has a 
single unknown. The final matrix after conducting EROs and back substitution 
leaves it in **row echelon form (REF).**

\begin{align}
  \left[\begin{array}{@{}ccc|c@{}}
    -2 & -4 & -6 & -2500 \\
    2 & 1 & 4 & 1800 \\ \hline
    0 & -3 & -2 & -700 
  \end{array}\right] \rightarrow
    \begin{bmatrix}
    1 & 2 & 3 & 150 \\
    0 & -3 & -2 & -700 \\
    0 & 1 & 1 & 300
  \end{bmatrix} \rightarrow
  \begin{bmatrix}
    1 & 2 & 3 & 150 \\
    0 & -3 & -2 & -700 \\
    0 & 0 & 1 & 200
  \end{bmatrix}
\end{align}

{{% notice note %}}
You can reduce the final matrix even further by applying EROs until you are left
with a single unknown (and a coefficient of 1) in each equation. Imagine a 
coefficient matrix with a diagonal line of 1s (typically from the upper left to 
bottom right) while every other element is listed as 0. This is known as 
**reduced row echelon form (RREF).** This allows a clearer way to find the 
solution to a system of linear equations without having to constantly perform 
back substitution on a matrix in REF (increasing the likelihood of mistakes). 
{{% /notice %}}

$x_3$ results in 200 from replacing the 3rd equation by finding the sum of equation 2 and 
multiplying the 3rd equation by 3 (to eliminate $x_2$). For the back substitution, 
plug in 200 into $x_3$ in equation 2 to solve for $x_2$ resulting in 100. Plug 
both values into their respective variables in equation 1 to get 450 for $x_1$. 
As a result, $(x_1, x_2, x_3) = (450, 100, 200)$. As our systems of linear equations 
begin to scale in size, it isn't practical to solve them by hand. Instead, we 
will use Python's scientific computing capabilities.

```{python}
import pandas as pd
data = pd.read_csv('sample.csv')
data.head()
```

{{% notice info %}}
The coefficient matrix is inversed to extract the coefficient values cleanly as lists.
{{% /notice %}}

```{python}
A = np.array([
  list(data['Weight']),
  list(data['Cars']),
  list(data['Mixture'])
])
b = np.array([1250, 1800, 300])

# Applies elementary row operations to a system of linear equations
def eroLEQ(matrix, constraints):
  A = matrix
  b = constraints
  
  # Check if the matrix has the same number of rows and columns
  if A.shape[0] == A.shape[1]:
    # Note: linalg.solve only accepts square matrices in the 1st parameter
    return np.linalg.solve(A, b)
  else:
    # lstsq for non-square matrices
    return np.linalg.lstsq(A, b, rcond = None)[0]
eroLEQ(A, b)
```

## Using Gurobi

```{python, echo = F}
from gurobipy import *
```

```{python}
T = Model("Commodity")

x1 = T.addVar(name="Coal")
x2 = T.addVar(name="Intermodal")
x3 = T.addVar(name="Agricultural")

T.setObjective(.01*x1 + .03*x2 + .04*x3, GRB.MAXIMIZE)

T.addConstr(1*x1 + 2*x2 + 3*x3 == 1250, "Weight")
T.addConstr(2*x1 + 1*x2 + 4*x3 == 1800, "Capacity")
T.addConstr(1*x2 + 1*x3 == 300, "Mixture")

T.optimize()

for i in T.getVars():
  print(f'{i.varName}: {i.x}.')
  
print(f'Optimal total revenue: {T.objVal}.')
```

## Terminology

* **Optimal Solution:** Any feasible solution that maximizes/minimizes the objective value.
* **Constraints**: Restrictions on the quantities of decision variables 
available.
* **Decision Variables:** Variables whose quantities affect the objective value 
in a decision-making process.
* **Dot Product:** The multiplication of two vectors with the same number of elements.
* **Row Echelon Form (REF):** According to @margalit_rabinoff_2019, a matrix is in REF if:
  + All nonzero rows are above rows of all 0s.
  + The leading nonzero entry of a row is to the *right* of the 1st nonzero entry of the row above.
  + Below the pivot, all entries are 0.
* **Reduced Row Echelon Form (RREF):** According to @margalit_rabinoff_2019, a 
matrix is in RREF if it is in REF and:
  + The pivot is equal to 1.
  + The pivot is the *only* nonzero entry in its column.
  
## References